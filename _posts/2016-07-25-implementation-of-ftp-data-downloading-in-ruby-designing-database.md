---
layout: post
comments: true
title: 'Implementation of FTP data downloading in Ruby. Part 1: designing database'
author: Krzysztof Maicher
date: 2016-07-25 19:39:04
categories:
tags: [Ruby, Rails, database, PostgreSQL]
---


### Introduction

Some time ago, I've created a Ruby on Rails application, which purpose was to present data (electric and gas meter readings) by charts and tables.
The application was acquiring that data from FTP.

You can read more about that app [here](/freelance-ruby-or-rails-application-development).

In these article I would like to present, how I've approached the database designing process.

### Analyzing the data

Data, that were stored on FTP by meters had following characteristics:

- meter readings in csv files on FTP server
- about 50 lines in each file
- one line represent one reading (two values, timestamp and a meter number)
- around 30 new files a day (can be more in future)
- that gives around 1.5k new readings every day

Over a year, the data would grow to _1.5k x 365_, that is around _0.5M_ records each year.
That is not that much and a properly designed PostgreSQL database can handle that amount of data without any problems.
My idea was to store that data in a relational database, that would allow me to easily access it later to view it as charts or tables.

### Designing database

I decided, that I wil access database by ActiveRecord models, because of its ease of use.
That is a classic approach, that is seen very often in Rails applications and it works quite well, when properly designed and used.
However, when it comes to creating new ActiveRecord models, my workflow may be different from other Rails developers.
I like to come up with a __database design upfront__ (in mind, on paper or in some tool) and after that, I start generating ActiveRecord models.
In other words I design database as a first step and as a second step, I build an __ActiveRecord models layer on top of it__.

I don't want to focus on the generator's commands. Every Rails developer knows them very well.
However not every Rails developer knows what's going on on a database level.

Can you read that schema?

![Database schema](/assets/gt_readings_db.png)

Schema was generated by a SQL Power Architect tool.

- There are 3 tables: `meters`, `readings`, `ftp_files`.
- Each table has a `id` row, which is a Primary Key (`[PK]`).
- There are various column types: `INTEGER`, `VARCHAR`, `TIMESTAMP`.
- Some columns have a non-null constraint (`[AK]`).
- A foreign key to is set between meters and readings tables (`[FK]`).

Could you write migrations, the result of which would be above schema?

### Unique indexes

What cannot be seen in above schema is the unique indexes.
I'd like describe them here.

My idea of ftp_files table was to store a list of file names, that have already been parsed.
It's pointless to allow keeping there doubled names. That is why I set a unique index on name column.

A Meter is identified by it's number.
There has to be only one meter with given number, that's why I decided to apply unique index on it as well.
Having a unique index on that column gives the assurance, that in application, every time a meter will be searched by number, there will be zero or exactly one meter found.

As it goes tor readings, it is highly undesirable to double readings in database, because charts created from them would present incorrect values.
One reading consists of 4 data. A timestamp, meter_id, value1, value2. How to prevent from saving double readings?
In PostgreSQL so called unique index on group can be applied.
There is a complete certainty, that when application for any reason (bug, my or some other developers mistake, or even a client's software bug) will try to save a double reading, an exception will be raised, which is very desirable.
I don't want to see double readings in that database. Unique indexes guarantees it and I can rely on that.

### Data integrity

It is highly probable, that that peace of database will keep it own [data integrity](https://en.wikipedia.org/wiki/Data_integrity) regardless of what will happen in application (unless of course the application will not modify the schema).

- Meters will not be doubled.
- Readings will not be doubled.
- There will be no readings without a value, timestamp or associated meter.
- There will be no orphaned records on meters-readings associacion.

Note, that the __security is kept on a database layer__. I other words, the database uses it own mechanisms to prevent loosing its own integrity.

That database doesn't need ActiveRecord layer to keep data integrity, which is desirable, bacause ActiveRecord validations (even though they are very useful and helpful) __doesn't guarantee the application's data integrity__.
That is because ActiveRecords validations can be omitted by using following methods:

`#save(validate: false)`, `.update_attribute`, `delete`, `.delete_all`, `.update_all`

Furthermore someone can modify database tables by raw SQL queries, or even by logging into psql console to run some updates or inserts. Finally, my own code can have bugs and for example can try to create double records.

### Summary

I see database as a layer, that can keep its data consistency by itself, without the use of ActiveRecord models.
I use ActiveRecord often and I think it is a solid piece of library. It speeds up the development process, comparing to not using any ORM system at all.
However using ActiveRecord models without a solidly designed database can lead to frustrations and to lots of maintenance work, when database will start loosing its integrity.

When you aim for a solid database design, learn about foreign keys and start using them.
Start using uniqueness validations not only in ActiveRecord models, but at a database layer as well (including unique indexes on groups or inside json fields).
Start specifying if a column can have null value or not and if not, then specify a default value.
